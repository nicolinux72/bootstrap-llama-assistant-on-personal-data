{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwp/5pC54vgw0dmUUSUr/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolinux72/bootstrap-llama-assistant-on-personal-data/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrap a LLaMa assistant on personal data\n",
        "*How to quickly fine tuning a LLM so that it responds about private data also.*\n",
        "\n",
        "You could find full article at [medium](https://medium.com/@nicolasanti_43152/bootstrap-a-llama-assistant-on-personal-data-2-2-16062fa5aa6d)\n",
        "\n",
        "We can then proceed to install libraries using the usual pip command."
      ],
      "metadata": {
        "id": "5G8nBFA-4scT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oREEjTXfpRvp"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude the application setup by logging in."
      ],
      "metadata": {
        "id": "YeTcQFV84-pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0FzPhYDpzae",
        "outputId": "a95a523f-9401-407c-d0ce-0f2379828ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load and execute the model, we will once again use the huggingface libraries."
      ],
      "metadata": {
        "id": "lPcAs5GW5Ozd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,BitsAndBytesConfig,pipeline\n",
        "\n",
        "\n",
        "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    config=AutoConfig.from_pretrained(model_name),\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=bfloat16),\n",
        "    device_map='cuda:0',\n",
        ")\n",
        "# enable model inference\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    task='text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # needed by langchain\n",
        "    # model params\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,  # creativity of responses: 0.0 none ->  1.0 max\n",
        "    repetition_penalty=1.1  # to avoid repeating output\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "yzfk8UCuaX_R",
        "outputId": "4737289c-5ec0-402a-9aa8-36673325581e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-96889d72ffd5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'meta-llama/Llama-2-7b-chat-hf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how to use the huggingface runtime we have previously configured within LangChain:"
      ],
      "metadata": {
        "id": "xKzI9uptNkjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "#to query the model simply use langchain abstraction, as below\n",
        "#llm(prompt=\"Rainbow colors are ...\")"
      ],
      "metadata": {
        "id": "p644zrUztRF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload your pdf and load it inside langchain."
      ],
      "metadata": {
        "id": "WSLNvW76EB1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader('./AZIENDA_IT_0120_low.pdf')\n",
        "documents = loader.load_and_split()\n",
        "#to see the loaded content, uncomment the follow line\n",
        "#documents[1]\n"
      ],
      "metadata": {
        "id": "CME0xgsDEETx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's possible to load document from web, also."
      ],
      "metadata": {
        "id": "H1B_Keo7HyCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"https://www.espn.com/\",\"https://www.brt.it/it/sostenibilita/\"])\n",
        "documents = loader.load() + documents\n",
        "\n",
        "#to see the loaded content, uncomment the follow line\n",
        "#documents[1]"
      ],
      "metadata": {
        "id": "TneW4_nRttL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ],
      "metadata": {
        "id": "hMypy6tjCI4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "#for text in texts:\n",
        "#  print(text)"
      ],
      "metadata": {
        "id": "TIc1-pCCt847"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAIS is a fast, in memory vector db from Meta."
      ],
      "metadata": {
        "id": "POLsxyocCxVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "# storing embeddings in the vector store\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "SE7e_A6CuRZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can build and use the chain."
      ],
      "metadata": {
        "id": "LPiUnQzVNsAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "tt = PromptTemplate.from_template(\"\"\"[INST] <<SYS>>Act like an italian so answer in italian only, be very clear and detailed<</SYS>>\n",
        "{content}[/INST]\"\"\")\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "query = \"Le destinazioni internazionali servite sono...\"\n",
        "result = chain({\"question\": tt.format(content=query), \"chat_history\": chat_history})\n",
        "print(result['answer'])\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "chat_history = [(query, result[\"answer\"])]\n",
        "query = \"Cos'è l'avviso di tentata consegna di BRT?\"\n",
        "result = chain({\"question\": tt.format(content=query), \"chat_history\": chat_history})\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5WKOkuIugh6",
        "outputId": "1b46b127-198e-4ed1-9df0-4abaa6cc7aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Le destinazioni internazionali servite sono numerose e diverse, ma qui di seguito vi elenchio alcune delle principali:\n",
            "\n",
            "* Europa: Bologna, Crespellano, Milano, Torino, Verona, Guidonia, Rovigo, Torino, Valdarno, Verona.\n",
            "* Francia: Parigi, Marsiglia, Nizza, Lione, Strasburgo.\n",
            "* Spagna: Madrid, Barcellona, Saragozza, Valencia, Bilbao, Barcellona.\n",
            "* Inghilterra: Londra, Manchester, Birmingham, Liverpool, Glasgow.\n",
            "* Germania: Berlino, Monaco di Baviera, Stoccarda, Amburgo, Colonia, Düsseldorf.\n",
            "* Paesi Bassi: Amsterdam, Rotterdam, Utrecht, Eindhoven, Groninga.\n",
            "* Svizzera: Zurigo, Ginevra, Losanna, Berna, Winterthur.\n",
            "* Belgio: Bruxelles, Anversa, Gand, Liegi, Charleroi.\n",
            "* Austria: Vienna, Graz, Salisburgh, Innsbruck, Linz.\n",
            "* Polonia: Varsavia, Cracovia, Poznań, Łódź, Kraków.\n",
            "* Rep. Ceca: Praga, Brno, Plzeň, Ostrava, Pilsen.\n",
            "* Slovacchia: Bratislava, Košice, Prešov, Trnava.\n",
            "* Ungheria: Budapest, Debrecen, Miskolc, Pécs, Szeged.\n",
            "* Grecia: Atene, Tessalonica, Salonicco, Patrasso, Rodi.\n",
            "* Turchia: Istanbul, Ankara, Izmir, Adana, Gaziantep.\n",
            "* Russia: Mosca, San Pietroburgo, Novosibirsk, Samara, Kazan'.\n",
            "* America Latina: Buenos Aires, Città del Messico, Lima, Santiago del Cile, San Paolo.\n",
            "* Stati Uniti d'America: New York, Los Angeles, Chicago, Houston, Filadelfia.\n",
            "* Canada: Toronto, Montreal, Vancouver, Calgary, Ottawa.\n",
            "* Australia: Sydney, Melbourne, Brisbane, Perth, Adelaide.\n",
            "* Nuova Zel\n",
            "----------------------\n",
            " I apologize, but I don't know the answer to your question as \"tentata consegna di BRT\" is not a commonly used term in logistics or delivery services. It may be a specific term used in a particular context or industry, but I'm not familiar with its meaning. Could you provide more context or information about where you encountered this term?\n"
          ]
        }
      ]
    }
  ]
}